# C:\Users\cherub\OneDrive\Desktop\test\langchain\QnA\src\agent.py
from typing import Annotated, List, Dict, Any, Optional
import uuid
from typing_extensions import TypedDict
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from src.tool import query_rag_agent
from langchain_core.tools import tool

# Define the state type for our graph
class State(TypedDict):
    messages: Annotated[list, add_messages]
    rag_results: Optional[Dict[str, Any]]
    thread_id: Optional[str]

# Initialize the LLM
llm = ChatOpenAI(model="gpt-4o-mini")

# Create tools list
tools = [query_rag_agent]
llm_with_tools = llm.bind_tools(tools)

# Create state graph
graph_builder = StateGraph(State)

# Define the agent nodes
def process_query(state: State):
    """Forward user query to RAG agent and store results"""
    messages = state["messages"]
    thread_id = state.get("thread_id")
    
    # Get the most recent user query
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage) or (hasattr(msg, "role") and msg.role == "user"):
            user_query = msg.content
            break
    else:
        # If no user query is found, return error
        return {"rag_results": {"bullet_points": ["No valid user query found"], "thread_id": thread_id or str(uuid.uuid4())}}
    
    # Call the RAG agent
    rag_response = query_rag_agent.invoke({"query": user_query, "thread_id": thread_id})
    
    # Update thread_id if it was generated by the RAG agent
    if not thread_id and "thread_id" in rag_response:
        thread_id = rag_response["thread_id"]
    
    # Return updated state
    return {"rag_results": rag_response, "thread_id": thread_id}

def generate_response(state: State):
    """Generate refined response based on RAG results"""
    messages = state["messages"]
    rag_results = state.get("rag_results", {})
    thread_id = state.get("thread_id")
    
    # Extract bullet points from RAG results
    bullet_points = rag_results.get("bullet_points", [])
    bullet_content = "\n".join(f"â€¢ {point}" for point in bullet_points)
    
    # Prepare system message with instructions
    system_message = {
        "role": "system", 
        "content": """You are a helpful, friendly assistant that provides comprehensive and well-structured responses.
        
        You will be given bullet points containing information retrieved from a knowledge base.
        Your task is to:
        
        1. Transform these bullet points into a coherent, natural, and conversational response
        2. Choose an appropriate format for the information (paragraphs, bullets, numbered list, etc.)
        3. Add relevant context and explanations where helpful
        4. Maintain a friendly, helpful tone throughout
        5. If the bullet points indicate there isn't enough information, acknowledge this and suggest alternatives
        6. If appropriate, offer a brief summary at the end
        
        The goal is to make the information more engaging and easier to understand than raw bullet points."""
    }
    
    # Add RAG results as context
    if bullet_points:
        system_message["content"] += f"\n\nHere are the bullet points from the knowledge base:\n{bullet_content}"
    else:
        system_message["content"] += "\n\nNo information was found in the knowledge base."
    
    # Create message list with system message and user query
    all_messages = [system_message] + messages
    
    # Invoke the LLM
    response = llm.invoke(all_messages)
    
    return {"messages": [response]}

# Add nodes to the graph
graph_builder.add_node("process_query", process_query)
graph_builder.add_node("generate_response", generate_response)

# Define the edges
graph_builder.add_edge(START, "process_query")
graph_builder.add_edge("process_query", "generate_response")
graph_builder.add_edge("generate_response", END)

# Initialize memory saver for persistence
memory = MemorySaver()

# Compile the graph
graph = graph_builder.compile()

# Function to invoke the agent with a query
def run_agent(query: str, thread_id: str = None) -> str:
    """
    Run the QnA agent with the given query and return a refined response.
    
    Args:
        query: The user's query
        thread_id: Thread ID for conversation persistence
        
    Returns:
        Refined response as a string
    """
    # Generate a random UUID if thread_id is not provided
    if thread_id is None:
        thread_id = str(uuid.uuid4())
    
    config = {"configurable": {"thread_id": thread_id}}
    
    # Create input state with user message
    input_state = {
        "messages": [{"role": "user", "content": query}],
        "rag_results": None,
        "thread_id": thread_id
    }
    
    # Run the graph
    result = graph.invoke(input_state, config)
    
    # Extract the assistant's response
    assistant_response = result["messages"][-1].content
    
    return assistant_response